{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChatBot_with_attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEJnam07CRF-"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow_text\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "import re \n",
        "from random import randint\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import typing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "aSErFeq3nPSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/movie_lines.txt', 'r', encoding='cp1252') as movie_lines:\n",
        "    lines = movie_lines.read().split('\\n')\n",
        "    \n",
        "movie_line_dict = {}\n",
        "for line in lines:\n",
        "    text = line.split(' +++$+++ ')[-1]\n",
        "    index = line.split(' +++$+++ ')[0]\n",
        "\n",
        "    movie_line_dict[index] = text\n",
        "        \n",
        "\n",
        "with open('/content/movie_conversations.txt', 'r', encoding='cp1252') as movie_conversation:\n",
        "    lines = movie_conversation.read().split('\\n')\n",
        "movie_conversation_list = [line.split(' +++$+++ ')[-1][1:-1].replace(\"'\", '').split(', ') for line in lines]"
      ],
      "metadata": {
        "id": "wCtaUGjSC00f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp, targ  = [], []\n",
        "\n",
        "for conv in movie_conversation_list:\n",
        "    for line in range(len(conv)-1): \n",
        "        inp.append(movie_line_dict[conv[line]])\n",
        "        targ.append(movie_line_dict[conv[line+1]])"
      ],
      "metadata": {
        "id": "l-q09GTaDLV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = randint(0, len(inp))\n",
        "print(f'Input_example: {inp[i]}')\n",
        "print(f'Target example: {targ[i]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlCwS1fBDqts",
        "outputId": "c043c97e-3e0d-414a-9b0b-22c2acb6e84a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input_example: Oh, sure.  Here, this can be mommy.\n",
            "Target example: Do you like to be with your mommy?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_lower_and_split_punct(text):\n",
        "  # Split accecented characters.\n",
        "  text = tf.strings.lower(text)\n",
        "  # Keep space, a to z, and select punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,]', '')\n",
        "  # Add spaces around punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[.?!,]', r' \\0 ')\n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text"
      ],
      "metadata": {
        "id": "RSvyxgZ4FJQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len_seq_enc = [len(seq) for seq in inp]\n",
        "len_seq_dec = [len(seq) for seq in targ]\n",
        "\n",
        "for percentile in [30, 40, 80, 90]:\n",
        "    print('{} percentile of encoder length sequances = {}'.\n",
        "          format(percentile, np.percentile(len_seq_enc, percentile)))\n",
        "    print('{} percentile of decoder length sequances = {}'.\n",
        "          format(percentile, np.percentile(len_seq_dec, percentile)), end = '\\n\\n')\n",
        "    \n",
        "inp_trimmed, targ_trimmed = [], []\n",
        "\n",
        "MIN_SEQ_LEN = 20\n",
        "MAX_SEQ_LEN = 120\n",
        "\n",
        "for i in range(len(inp)):\n",
        "    if (len(inp[i]) >= MIN_SEQ_LEN and len(inp[i]) <= MAX_SEQ_LEN)\\\n",
        "    and (len(targ[i]) >= MIN_SEQ_LEN and len(targ[i]) <= MAX_SEQ_LEN):\n",
        "        inp_trimmed.append(inp[i])\n",
        "        targ_trimmed.append(targ[i])\n",
        "\n",
        "\n",
        "print(f'sample shape Input = {len(inp_trimmed)}')\n",
        "print(f'sample shape Tragte = {len(targ_trimmed)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-pEIQ31HeXb",
        "outputId": "17a1c767-abe1-4ce4-b1fa-2dfb11bcb9c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30 percentile of encoder length sequances = 21.0\n",
            "30 percentile of decoder length sequances = 21.0\n",
            "\n",
            "40 percentile of encoder length sequances = 27.0\n",
            "40 percentile of decoder length sequances = 28.0\n",
            "\n",
            "80 percentile of encoder length sequances = 78.0\n",
            "80 percentile of decoder length sequances = 81.0\n",
            "\n",
            "90 percentile of encoder length sequances = 117.0\n",
            "90 percentile of decoder length sequances = 122.0\n",
            "\n",
            "sample shape Input = 89803\n",
            "sample shape Tragte = 89803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 30000\n",
        "\n",
        "result_inp = inp_trimmed[:num_samples]\n",
        "result_targ = targ_trimmed[:num_samples]"
      ],
      "metadata": {
        "id": "t9Hmoyq-Isz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = randint(0, len(result_inp))\n",
        "print(f'Input example: {result_inp[i]}')\n",
        "print(f'Target example: {result_targ[i]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD2nC1OmKPV-",
        "outputId": "9735c84d-fe0d-4d2f-8aff-f9252c9c2fcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input example: \"Thunder Road,\" Bruce Springsteen, from Born to Run. \"Smells Like Teen Spirit,\" Nirvana, Nevermind.\n",
            "Target example: Oh no, Rob, that's not obvious enough. Not at all.  Dick, did you hear that?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(inp)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((result_inp, result_targ)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "u-PwxDGL_R9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = randint(0, len(inp))\n",
        "example_text = tf.constant(inp[i])\n",
        "\n",
        "print(example_text.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS4KLpNG_pnh",
        "outputId": "93e514dd-6af0-4bf3-d336-ed1369328f4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"Well, even if I'm not the wolfman, I am crazy enough to do something like that. I mean, here I sit in Leicester Square talking to a corpse. I'm glad to see you, Jack.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for example_input_batch, example_target_batch in dataset.take(1):\n",
        "  print(example_input_batch[:5])\n",
        "  print()\n",
        "  print(example_target_batch[:5])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoF1lUvT_YfF",
        "outputId": "55530e88-74a4-47f2-d9c8-de73e2ba09ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'Vanessa. Shit, you better get out of here.'\n",
            " b'Promise me you will never leave me.' b'What can I do about it?'\n",
            " b'It was very silly.... I would be ashamed to wear it here.'\n",
            " b'I lived there for about nine months.'], shape=(5,), dtype=string)\n",
            "\n",
            "tf.Tensor(\n",
            "[b'Why? What the big deal?' b'I cannot promise you that.'\n",
            " b\"You can't do jack shit... unless you learn your evil powers.\"\n",
            " b'As beautiful as that? What else? Come, tell me.'\n",
            " b'You lived in Japan, when?'], shape=(5,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(example_text.numpy().decode())\n",
        "print(tf_lower_and_split_punct(example_text).numpy().decode())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oR0EVcSeAGcI",
        "outputId": "5dd4983c-0db4-417b-ab2c-1610db4c46fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Well, even if I'm not the wolfman, I am crazy enough to do something like that. I mean, here I sit in Leicester Square talking to a corpse. I'm glad to see you, Jack.\n",
            "[START] well ,  even if im not the wolfman ,  i am crazy enough to do something like that .  i mean ,  here i sit in leicester square talking to a corpse .  im glad to see you ,  jack . [END]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_VOCAB_SIZE = 3000\n",
        "\n",
        "text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=MAX_VOCAB_SIZE)"
      ],
      "metadata": {
        "id": "RgJs_YGeI5EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_processor.adapt(result_inp)\n",
        "\n",
        "text_processor.get_vocabulary()[1000:1007]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCUcOPuNJOJ8",
        "outputId": "f7bd0e1f-3663-42b3-fa88-284a638590b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['taste', 'strong', 'stick', 'station', 'single', 'ring', 'quiet']"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_tokens = text_processor(example_input_batch)\n",
        "example_tokens[:3, :10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlK99LakJnOP",
        "outputId": "91176688-2402-4ff1-d69f-882a3a9c2152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 10), dtype=int64, numpy=\n",
              "array([[   3,    8,  582,   18,   29,  219,   20, 1792,    2, 1792],\n",
              "       [   3,   75,    5,   47,   22,    1,  358,   17, 1148,    2],\n",
              "       [   3,    9,  177,   38, 1285,   14,   20,   28,    1,  567]])>"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_vocab = np.array(text_processor.get_vocabulary())\n",
        "tokens = input_vocab[example_tokens[0].numpy()]\n",
        "' '.join(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "03DqP9HNLY70",
        "outputId": "9c7e6728-e566-4bb9-fc34-5be1a20fdfa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[START] i drive and my name is larry . larry from the states . [END]                                             '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(example_tokens)\n",
        "plt.title('Token IDs')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(example_tokens != 0)\n",
        "plt.title('Mask')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "Q1ueklCDLfag",
        "outputId": "8a0cde6b-bb97-4da0-fcdf-8405d0d5ee4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Mask')"
            ]
          },
          "metadata": {},
          "execution_count": 106
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMB_DIM = 256\n",
        "UNITS = 512"
      ],
      "metadata": {
        "id": "HywxHxYNLwMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderInput(typing.NamedTuple):\n",
        "    new_tokens: typing.Any\n",
        "    enc_output: typing.Any\n",
        "    mask: typing.Any\n",
        "\n",
        "class DecoderOutput(typing.NamedTuple):\n",
        "    logits: typing.Any\n",
        "    attention_weights: typing.Any\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.enc_units = enc_units\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
        "                                               EMB_DIM)\n",
        "\n",
        "\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, tokens, state=None):\n",
        "\n",
        "    vectors = self.embedding(tokens)\n",
        "\n",
        "    output, state = self.gru(vectors, initial_state=state)\n",
        "\n",
        "    return output, state\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super().__init__()\n",
        "\n",
        "    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "\n",
        "    self.attention = tf.keras.layers.AdditiveAttention()\n",
        "\n",
        "  def call(self, query, value, mask):\n",
        "\n",
        "    w1_query = self.W1(query)\n",
        "\n",
        "    w2_key = self.W2(value)\n",
        "\n",
        "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "    value_mask = mask\n",
        "\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        inputs = [w1_query, value, w2_key],\n",
        "        mask=[query_mask, value_mask],\n",
        "        return_attention_scores = True,\n",
        "    )\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.dec_units = dec_units\n",
        "    self.output_vocab_size = output_vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
        "                                    use_bias=False)\n",
        "\n",
        "    self.fc = tf.keras.layers.Dense(self.output_vocab_size)\n",
        "\n",
        "  def call(self,inputs: DecoderInput,state=None) -> typing.Tuple[DecoderOutput, tf.Tensor]:\n",
        "\n",
        "    vectors = self.embedding(inputs.new_tokens)\n",
        "\n",
        "    rnn_output, state = self.gru(vectors, initial_state=state)\n",
        "\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
        "\n",
        "    context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
        "\n",
        "    attention_vector = self.Wc(context_and_rnn_output)\n",
        "\n",
        "    logits = self.fc(attention_vector)\n",
        "\n",
        "    return DecoderOutput(logits, attention_weights), state\n",
        "\n"
      ],
      "metadata": {
        "id": "50N9qHAwLxDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "\n",
        "  def __call__(self, y_true, y_pred):\n",
        "\n",
        "    loss = self.loss(y_true, y_pred)\n",
        "    \n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "\n",
        "    loss *= mask\n",
        "    \n",
        "    return tf.reduce_sum(loss)"
      ],
      "metadata": {
        "id": "LQCiNno5zIp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainChatBot(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units,\n",
        "               text_processor,\n",
        "               use_tf_function=True):\n",
        "    super().__init__()\n",
        "\n",
        "    encoder = Encoder(text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "    decoder = Decoder(text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.text_processor = text_processor\n",
        "    self.use_tf_function = use_tf_function\n",
        "\n",
        "  def train_step(self, inputs):\n",
        "    if self.use_tf_function:\n",
        "      return self._tf_train_step(inputs)\n",
        "    else:\n",
        "      return self._train_step(inputs)\n",
        "\n",
        "  @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
        "                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
        "  def _tf_train_step(self, inputs):\n",
        "    return self._train_step(inputs)\n",
        "\n",
        "  def _preprocess(self, input_text, target_text):\n",
        "\n",
        "\n",
        "    input_tokens = self.text_processor(input_text)\n",
        "    target_tokens = self.text_processor(target_text)\n",
        "\n",
        "    input_mask = (input_tokens != 0)\n",
        "\n",
        "    target_mask = (target_tokens != 0)\n",
        "\n",
        "    return input_tokens, input_mask, target_tokens, target_mask\n",
        "\n",
        "  def _train_step(self, inputs):\n",
        "    input_text, target_text = inputs  \n",
        "\n",
        "    (input_tokens, input_mask,\n",
        "    target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
        "\n",
        "    max_target_length = tf.shape(target_tokens)[1]\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "      enc_output, enc_state = self.encoder(input_tokens)\n",
        "\n",
        "      dec_state = enc_state\n",
        "      loss = tf.constant(0.0)\n",
        "\n",
        "      for t in tf.range(max_target_length-1):\n",
        "\n",
        "        new_tokens = target_tokens[:, t:t+2]\n",
        "        step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
        "                                              enc_output, dec_state)\n",
        "        loss = loss + step_loss\n",
        "\n",
        "      average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
        "\n",
        "    variables = self.trainable_variables \n",
        "    gradients = tape.gradient(average_loss, variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return {'batch_loss': average_loss}\n",
        "\n",
        "  def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
        "    input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
        "\n",
        "    decoder_input = DecoderInput(new_tokens=input_token,\n",
        "                                enc_output=enc_output,\n",
        "                                mask=input_mask)\n",
        "\n",
        "    dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
        "\n",
        "    y = target_token\n",
        "    y_pred = dec_result.logits\n",
        "    step_loss = self.loss(y, y_pred)\n",
        "\n",
        "    return step_loss, dec_state"
      ],
      "metadata": {
        "id": "NH3zEAAbzjs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ChatBot = TrainChatBot(\n",
        "    EMB_DIM, UNITS,\n",
        "    text_processor=text_processor,\n",
        "    use_tf_function=True)\n",
        "\n",
        "ChatBot.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")"
      ],
      "metadata": {
        "id": "jR2EsJKp1Mgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchLogs(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, key):\n",
        "    self.key = key\n",
        "    self.logs = []\n",
        "\n",
        "  def on_train_batch_end(self, n, logs):\n",
        "    self.logs.append(logs[self.key])\n",
        "\n",
        "batch_loss = BatchLogs('batch_loss')"
      ],
      "metadata": {
        "id": "ft_qoFUn5sNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ChatBot.fit(dataset, epochs = 3, callbacks = [batch_loss])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSdotTaA_Eph",
        "outputId": "ea96446e-5c05-44b0-b384-8427e8c9b62d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "469/469 [==============================] - 1781s 4s/step - batch_loss: 4.5046\n",
            "Epoch 2/3\n",
            "469/469 [==============================] - 1786s 4s/step - batch_loss: 3.8957\n",
            "Epoch 3/3\n",
            "469/469 [==============================] - 1670s 4s/step - batch_loss: 3.6441\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f50bdb28290>"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatBotAnsw(tf.Module):\n",
        "\n",
        "  def __init__(self, encoder, decoder, text_processor):\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.text_processor = text_processor\n",
        "\n",
        "    self.output_token_string_from_index = (\n",
        "        tf.keras.layers.StringLookup(\n",
        "            vocabulary=text_processor.get_vocabulary(),\n",
        "            mask_token='',\n",
        "            invert=True))\n",
        "\n",
        "    index_from_string = tf.keras.layers.StringLookup(\n",
        "        vocabulary=text_processor.get_vocabulary(), mask_token='')\n",
        "    token_mask_ids = index_from_string(['', '[UNK]', '[START]']).numpy()\n",
        "\n",
        "    token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
        "    token_mask[np.array(token_mask_ids)] = True\n",
        "\n",
        "    self.token_mask = token_mask\n",
        "\n",
        "    self.start_token = index_from_string(tf.constant('[START]'))\n",
        "    self.end_token = index_from_string(tf.constant('[END]'))\n",
        "\n",
        "  def tokens_to_text(self, result_tokens):\n",
        "\n",
        "    result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
        "    result_text = tf.strings.reduce_join(result_text_tokens,\n",
        "                                        axis=1, separator=' ')\n",
        "    result_text = tf.strings.strip(result_text)\n",
        "    return result_text\n",
        "\n",
        "  def sample(self, logits, temperature):\n",
        "\n",
        "    token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
        "    logits = tf.where(self.token_mask, -np.inf, logits)\n",
        "    if temperature == 0.0:\n",
        "      new_tokens = tf.argmax(logits, axis=-1)\n",
        "    else: \n",
        "      logits = tf.squeeze(logits, axis=1)\n",
        "      new_tokens = tf.random.categorical(logits/temperature,\n",
        "                                          num_samples=1)\n",
        "    return new_tokens\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])  \n",
        "  def get_answer(self,\n",
        "                       input_text, *,\n",
        "                       max_length=50,\n",
        "                       return_attention=True,\n",
        "                       temperature=1.0):\n",
        "    batch_size = tf.shape(input_text)[0]\n",
        "    input_tokens = self.text_processor(input_text)\n",
        "    enc_output, enc_state = self.encoder(input_tokens)\n",
        "\n",
        "    dec_state = enc_state\n",
        "    new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "\n",
        "    result_tokens = []\n",
        "    attention = []\n",
        "    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "      dec_input = DecoderInput(new_tokens=new_tokens,\n",
        "                              enc_output=enc_output,\n",
        "                              mask=(input_tokens!=0))\n",
        "\n",
        "      dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
        "\n",
        "      attention.append(dec_result.attention_weights)\n",
        "\n",
        "      new_tokens = self.sample(dec_result.logits, temperature)\n",
        "\n",
        "      done = done | (new_tokens == self.end_token)\n",
        "\n",
        "      new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
        "\n",
        "      result_tokens.append(new_tokens)\n",
        "\n",
        "      if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "        break\n",
        "\n",
        "    result_tokens = tf.concat(result_tokens, axis=-1)\n",
        "    result_text = self.tokens_to_text(result_tokens)\n",
        "\n",
        "    if return_attention:\n",
        "      attention_stack = tf.concat(attention, axis=1)\n",
        "      return { 'text': result_text, 'attention': attention_stack}\n",
        "    else:\n",
        "      return {'text': result_text}\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "-aF7EaLWA0LW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_input = tf.constant(result_inp[:5])\n",
        "simple_target = tf.constant(result_targ[:5])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "chatbotansw = ChatBotAnsw(\n",
        "    encoder=ChatBot.encoder,\n",
        "    decoder=ChatBot.decoder,\n",
        "    text_processor=text_processor,\n",
        ")\n",
        "\n",
        "ans = chatbotansw.get_answer(input_text = simple_input, )\n",
        "\n",
        "for i in range(simple_input.shape[0]):\n",
        "  print(f'\\nInput text: {simple_input[i]}')\n",
        "  print(f\"ChatBot`s answer text: {ans['text'][i]}\")\n",
        "  print(f\"Tareget text : {simple_target[i]}\",\n",
        "        end = '\\n\\n=====================================================================\\n\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMzM7Tx0LdOt",
        "outputId": "ab1683a5-212a-4fd4-a150-78965a67f647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:5 out of the last 6 calls to <function ChatBotAnsw.get_answer at 0x7f50c2513170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\n",
            "Input text: b\"Well, I thought we'd start with pronunciation, if that's okay with you.\"\n",
            "ChatBot`s answer text: b'what did you name ?'\n",
            "Tareget text : b'Not the hacking and gagging and spitting part.  Please.'\n",
            "\n",
            "=====================================================================\n",
            "\n",
            "\n",
            "Input text: b'Not the hacking and gagging and spitting part.  Please.'\n",
            "ChatBot`s answer text: b'youll call me wrong , there . wheres in here ?'\n",
            "Tareget text : b\"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\"\n",
            "\n",
            "=====================================================================\n",
            "\n",
            "\n",
            "Input text: b'Gosh, if only we could find Kat a boyfriend...'\n",
            "ChatBot`s answer text: b'didnt go across best . thats why you mind now ?'\n",
            "Tareget text : b'Let me see what I can do.'\n",
            "\n",
            "=====================================================================\n",
            "\n",
            "\n",
            "Input text: b\"C'esc ma tete. This is my head\"\n",
            "ChatBot`s answer text: b'nothing away will make later any choice , you know so tell you , i called . see im not going to promise .'\n",
            "Tareget text : b\"Right.  See?  You're ready for the quiz.\"\n",
            "\n",
            "=====================================================================\n",
            "\n",
            "\n",
            "Input text: b'How is our little Find the Wench A Date plan progressing?'\n",
            "ChatBot`s answer text: b'you should follow me , would you if these are not so important .'\n",
            "Tareget text : b\"Well, there's someone I think might be --\"\n",
            "\n",
            "=====================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ChatBot.save_weights('/content/drive/MyDrive/ChatBot with attention/', save_format = 'tf')"
      ],
      "metadata": {
        "id": "tVPB0DDJetBG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}